<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>无重复字符的最长子串 LC3</title>
      <link href="/2023/04/03/leetcode/wu-chong-fu-zi-fu-de-zui-chang-zi-chuan/"/>
      <url>/2023/04/03/leetcode/wu-chong-fu-zi-fu-de-zui-chang-zi-chuan/</url>
      
        <content type="html"><![CDATA[<h1 id="LeetCode3-无重复字符的最长子串"><a href="#LeetCode3-无重复字符的最长子串" class="headerlink" title="LeetCode3 无重复字符的最长子串"></a>LeetCode3 无重复字符的最长子串</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个字符串 <code>s</code> ，请你找出其中不含有重复字符的 <strong>最长子串</strong> 的长度。</p><p><strong>示例 1:</strong></p><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">输入: s = "abcabcbb"输出: 3 解释: 因为无重复字符的最长子串是 "abc"，所以其长度为 3。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>示例 2:</p><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">输入: s = "bbbbb"输出: 1解释: 因为无重复字符的最长子串是 "b"，所以其长度为 1。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>示例 3:</p><pre class="line-numbers language-markdown" data-language="markdown"><code class="language-markdown">输入: s = "pwwkew"输出: 3解释: 因为无重复字符的最长子串是 "wke"，所以其长度为 3。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>提示：</strong></p><ul><li><code>0 &lt;= s.length &lt;= 5 * 10^4</code></li><li><code>s</code> 由英文字母、数字、符号和空格组成</li></ul><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><p><strong>滑动窗口</strong></p><p>时间复杂度：$O(N)$</p><p>空间复杂度：$O(∣Σ∣)$，$∣Σ∣$表示字符集的大小</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span><span class="token keyword">public</span><span class="token operator">:</span>    <span class="token keyword">int</span> <span class="token function">lengthOfLongestSubstring</span><span class="token punctuation">(</span>string s<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment">// 哈希集合，记录每个字符是否出现过</span>        unordered_set<span class="token operator">&lt;</span><span class="token keyword">char</span><span class="token operator">&gt;</span> lookup<span class="token punctuation">;</span>        <span class="token keyword">int</span> n <span class="token operator">=</span> s<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment">// 右指针，初始值为 -1，相当于我们在字符串的左边界的左侧，还没有开始移动</span>        <span class="token keyword">int</span> right <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>        <span class="token keyword">int</span> maxStr <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>        <span class="token comment">// 枚举左指针的位置，初始值隐性地表示为 -1</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token comment">// 左指针向右移动一格，移除一个字符</span>                lookup<span class="token punctuation">.</span><span class="token function">erase</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment">// 判断此时第rk+1个字符是否存在于哈希表中，并且没有超出边界</span>            <span class="token keyword">while</span> <span class="token punctuation">(</span>right <span class="token operator">&lt;</span> n <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">(</span>lookup<span class="token punctuation">.</span><span class="token function">count</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>right <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token comment">// 不断地移动右指针</span>                lookup<span class="token punctuation">.</span><span class="token function">insert</span><span class="token punctuation">(</span>s<span class="token punctuation">[</span>right <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                right<span class="token operator">++</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token keyword">int</span> curStr <span class="token operator">=</span> right <span class="token operator">-</span> i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>            <span class="token comment">// 第 i 到 right 个字符是一个极长的无重复字符子串</span>            maxStr <span class="token operator">=</span> <span class="token function">max</span><span class="token punctuation">(</span>maxStr<span class="token punctuation">,</span> curStr<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> maxStr<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="题目链接"><a href="#题目链接" class="headerlink" title="题目链接"></a>题目链接</h2><p><a href="https://leetcode.cn/problems/longest-substring-without-repeating-characters/">https://leetcode.cn/problems/longest-substring-without-repeating-characters/</a></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode100 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode100 </tag>
            
            <tag> AC </tag>
            
            <tag> Medium </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文List</title>
      <link href="/2023/03/26/lun-wen-list/"/>
      <url>/2023/03/26/lun-wen-list/</url>
      
        <content type="html"><![CDATA[<h1 id="论文List"><a href="#论文List" class="headerlink" title="论文List"></a>论文List</h1><h2 id="1-图方向经典"><a href="#1-图方向经典" class="headerlink" title="1. 图方向经典"></a>1. 图方向经典</h2><ol><li>Kipf T N, Welling M. Semi-supervised classification with graph convolutional networks[J]. arXiv preprint arXiv:1609.02907, 2016.</li><li>Veličković P, Cucurull G, Casanova A, et al. Graph attention networks[J]. arXiv preprint arXiv:1710.10903, 2017.</li><li>Yin Y, Wang Q, Huang S, et al. Autogcl: Automated graph contrastive learning via learnable view generators[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(8): 8892-8900.</li><li>Wang X, Ji H, Shi C, et al. Heterogeneous graph attention network[C]//The world wide web conference. 2019: 2022-2032.</li><li>Wu J, Wang X, Feng F, et al. Self-supervised graph learning for recommendation[C]//Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 2021: 726-735.</li><li>Yu J, Yin H, Xia X, et al. Are graph augmentations necessary? simple graph contrastive learning for recommendation[C]//Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2022: 1294-1303.</li></ol><h2 id="2-链接预测"><a href="#2-链接预测" class="headerlink" title="2. 链接预测"></a>2. 链接预测</h2><ol><li>Cai L, Li J, Wang J, et al. Line graph neural networks for link prediction[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021, 44(9): 5103-5113.</li><li>Liu X, Li X, Fiumara G, et al. Link prediction approach combined graph neural network with capsule network[J]. Expert Systems with Applications, 2023, 212: 118737.</li><li>Wang H, Yin H, Zhang M, et al. Equivariant and stable positional encoding for more powerful graph neural networks[J]. arXiv preprint arXiv:2203.00199, 2022.</li><li>Chamberlain B P, Shirobokov S, Rossi E, et al. Graph Neural Networks for Link Prediction with Subgraph Sketching[J]. arXiv preprint arXiv:2209.15486, 2022.</li><li>Pan L, Shi C, Dokmanić I. Neural link prediction with walk pooling[J]. arXiv preprint arXiv:2110.04375, 2021.</li></ol>]]></content>
      
      
      <categories>
          
          <category> Link Prediction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Learning </tag>
            
            <tag> Link Prediction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两数相加 LC2</title>
      <link href="/2023/03/26/leetcode/liang-shu-xiang-jia/"/>
      <url>/2023/03/26/leetcode/liang-shu-xiang-jia/</url>
      
        <content type="html"><![CDATA[<h1 id="LeetCode2-两数求和"><a href="#LeetCode2-两数求和" class="headerlink" title="LeetCode2 两数求和"></a>LeetCode2 两数求和</h1><h2 id="基础知识——链表"><a href="#基础知识——链表" class="headerlink" title="基础知识——链表"></a>基础知识——链表</h2><h3 id="1-链表的基本模板"><a href="#1-链表的基本模板" class="headerlink" title="1. 链表的基本模板"></a>1. 链表的基本模板</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">// Definition for singly-linked list.</span><span class="token keyword">struct</span> <span class="token class-name">ListNode</span> <span class="token punctuation">{</span>    <span class="token keyword">int</span> val<span class="token punctuation">;</span> <span class="token comment">// 数据域</span>    ListNode <span class="token operator">*</span>next<span class="token punctuation">;</span> <span class="token comment">// 指数域</span>    <span class="token function">ListNode</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token function">val</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">next</span><span class="token punctuation">(</span><span class="token keyword">nullptr</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token function">ListNode</span><span class="token punctuation">(</span><span class="token keyword">int</span> x<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token function">val</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">next</span><span class="token punctuation">(</span><span class="token keyword">nullptr</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token function">ListNode</span><span class="token punctuation">(</span><span class="token keyword">int</span> x<span class="token punctuation">,</span> ListNode <span class="token operator">*</span>next<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token function">val</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">next</span><span class="token punctuation">(</span>next<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-创建链表"><a href="#2-创建链表" class="headerlink" title="2. 创建链表"></a>2. 创建链表</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">// 创建链表</span>ListNode<span class="token operator">*</span> <span class="token function">creation</span><span class="token punctuation">(</span><span class="token keyword">int</span> n<span class="token punctuation">)</span> <span class="token punctuation">{</span>    ListNode <span class="token operator">*</span>l1_head <span class="token operator">=</span> <span class="token keyword">new</span> ListNode<span class="token punctuation">;</span> <span class="token comment">// 头节点 不存储数据</span>    l1_head <span class="token operator">-&gt;</span> next <span class="token operator">=</span> <span class="token keyword">nullptr</span><span class="token punctuation">;</span> <span class="token comment">// 指数域指向空</span>    ListNode<span class="token operator">*</span> pre <span class="token operator">=</span> l1_head<span class="token punctuation">;</span> <span class="token comment">// 定义pre，最开始的pre等于l1_head</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">&lt;=</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment">// 每次循环创建一个新的节点</span>        ListNode<span class="token operator">*</span> temp <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token function">ListNode</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment">// pre的指数域指向的下一个节点temp，把pre和temp连接起来</span>        pre <span class="token operator">-&gt;</span> next <span class="token operator">=</span> temp<span class="token punctuation">;</span>        <span class="token comment">// 把temp赋给pre，此时的pre就是最后的temp</span>        pre <span class="token operator">=</span> temp<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> l1_head<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-输出链表"><a href="#3-输出链表" class="headerlink" title="3. 输出链表"></a>3. 输出链表</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">// 输出链表</span><span class="token keyword">void</span> <span class="token function">outputList</span><span class="token punctuation">(</span>ListNode<span class="token operator">*</span> l1_head<span class="token punctuation">)</span> <span class="token punctuation">{</span>    ListNode<span class="token operator">*</span> temp <span class="token operator">=</span> l1_head <span class="token operator">-&gt;</span> next<span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span>temp <span class="token operator">!=</span> <span class="token keyword">nullptr</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        cout<span class="token operator">&lt;&lt;</span>temp<span class="token operator">&lt;&lt;</span><span class="token string">" "</span><span class="token operator">&lt;&lt;</span>temp<span class="token operator">-&gt;</span>val<span class="token operator">&lt;&lt;</span>endl<span class="token punctuation">;</span>        temp <span class="token operator">=</span> temp<span class="token operator">-&gt;</span>next<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-主函数"><a href="#4-主函数" class="headerlink" title="4. 主函数"></a>4. 主函数</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment">// 生成一个五个节点的链表</span>    ListNode<span class="token operator">*</span> l1_head <span class="token operator">=</span> <span class="token function">creation</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">outputList</span><span class="token punctuation">(</span>l1_head<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">system</span><span class="token punctuation">(</span><span class="token string">"pause"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2>]]></content>
      
      
      <categories>
          
          <category> LeetCode100 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode100 </tag>
            
            <tag> 链表 </tag>
            
            <tag> Medium </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>两数之和 LC1</title>
      <link href="/2023/03/26/leetcode/liang-shu-zhi-he/"/>
      <url>/2023/03/26/leetcode/liang-shu-zhi-he/</url>
      
        <content type="html"><![CDATA[<h1 id="LeetCode1-两数之和"><a href="#LeetCode1-两数之和" class="headerlink" title="LeetCode1 两数之和"></a>LeetCode1 两数之和</h1><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个整数数组 $nums$ 和一个整数目标值$target$，请你在该数组中找出和为目标值$target$的那两个整数，并返回它们的数组下标。</p><p>你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</p><p>你可以按任意顺序返回答案。</p><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode.cn/problems/two-sum">https://leetcode.cn/problems/two-sum</a><br>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230326151410087.png" alt="" style="zoom:80%;"></p><p>利用哈希表，查找x = target - nums[i]，如果此时这个x在哈希表中，证明查找成功。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span><span class="token keyword">public</span><span class="token operator">:</span>    vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">&gt;</span> <span class="token function">twoSum</span><span class="token punctuation">(</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">&gt;</span><span class="token operator">&amp;</span> nums<span class="token punctuation">,</span> <span class="token keyword">int</span> target<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment">// unordered_map 无序哈希表    </span>    unordered_map<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token operator">&gt;</span> hashtable<span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> nums<span class="token punctuation">.</span><span class="token function">size</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> i<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">int</span> x <span class="token operator">=</span> target <span class="token operator">-</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>        <span class="token keyword">auto</span> it <span class="token operator">=</span> hashtable<span class="token punctuation">.</span><span class="token function">find</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span>it <span class="token operator">!=</span> hashtable<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>            <span class="token keyword">return</span> <span class="token punctuation">{</span>it<span class="token operator">-&gt;</span>second<span class="token punctuation">,</span> i<span class="token punctuation">}</span><span class="token punctuation">;</span>            <span class="token comment">// it-&gt;second是对应的value</span>        <span class="token punctuation">}</span>        <span class="token comment">// key是nums[i]的值</span>        <span class="token comment">// value是数组对应下标</span>        hashtable<span class="token punctuation">[</span>nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> i<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>时间复杂度为$O(N)$</p><p>空间复杂度为$O(N)$</p><h3 id="C-unordered-map容器的成员方法"><a href="#C-unordered-map容器的成员方法" class="headerlink" title="C++ unordered_map容器的成员方法"></a>C++ unordered_map容器的成员方法</h3><div class="table-container"><table><thead><tr><th>begin()</th><th>返回指向容器中第一个键值对的正向迭代器。</th></tr></thead><tbody><tr><td>end()</td><td>返回指向容器中最后一个键值对之后位置的正向迭代器。</td></tr><tr><td>size()</td><td>返回当前容器中存有键值对的个数。</td></tr><tr><td>max_size()</td><td>返回容器所能容纳键值对的最大个数，不同的操作系统，其返回值亦不相同。</td></tr><tr><td>find(key)</td><td>查找以 key 为键的键值对，如果找到，则返回一个指向该键值对的正向迭代器；反之，则返回一个指向容器中最后一个键值对之后位置的迭代器（如果 end() 方法返回的迭代器）。</td></tr><tr><td>at(key)</td><td>返回容器中存储的键 key 对应的值，如果 key 不存在，则会抛出 out_of_range 异常。</td></tr></tbody></table></div><p><a href="http://c.biancheng.net/view/7231.html">C++ STL unordered_map容器用法详解 (biancheng.net)</a></p>]]></content>
      
      
      <categories>
          
          <category> LeetCode100 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode100 </tag>
            
            <tag> AC </tag>
            
            <tag> Easy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hello-world</title>
      <link href="/2023/03/26/hello-world/"/>
      <url>/2023/03/26/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> Hello </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hello </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GCN</title>
      <link href="/2023/03/26/gcn/"/>
      <url>/2023/03/26/gcn/</url>
      
        <content type="html"><![CDATA[<h1 id="GCN-图卷积神经网络"><a href="#GCN-图卷积神经网络" class="headerlink" title="GCN 图卷积神经网络"></a>GCN 图卷积神经网络</h1><p><em>无穷的远方，无数的人们，都和我有关。——鲁迅《且介亭杂文末集·这也是生活》</em></p><h2 id="1-将节点映射为d维向量"><a href="#1-将节点映射为d维向量" class="headerlink" title="1. 将节点映射为d维向量"></a>1. 将节点映射为d维向量</h2><p>图神经网络的基本原理是：</p><p><strong>将图中节点编码映射成一个低维连续稠密的d维向量</strong></p><p>d可以为128、256…</p><p>这个向量可以反映在原图的属性关系</p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325195204078.png" alt="图1.1 节点映射为d维向量" style="zoom:80%;"></p><h2 id="2-计算图"><a href="#2-计算图" class="headerlink" title="2. 计算图"></a>2. 计算图</h2><h3 id="2-1-通过局部邻域构建计算图"><a href="#2-1-通过局部邻域构建计算图" class="headerlink" title="2.1 通过局部邻域构建计算图"></a>2.1 通过局部邻域构建计算图</h3><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325152228320.png" alt="图2.1.1 计算图" style="zoom:80%;"></p><p>每个节点可以分别构建自己的计算图</p><p>例如图2.1中的图，每个节点对应的计算图如下所示</p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325152531870.png" alt="图2.1.2 每个节点对应的计算图" style="zoom:67%;"></p><p>在计算图中，所有的黑色神经网络共享一套参数，所有的白色神经网络共享一套参数。</p><h3 id="2-2-图神经网络层数"><a href="#2-2-图神经网络层数" class="headerlink" title="2.2 图神经网络层数"></a>2.2 图神经网络层数</h3><p><strong>图神经网络的层数，是计算图的层数，而不是神经网络的层数。</strong></p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325152854348.png" alt="图2.2.1 图神经网络的层数" style="zoom:80%;"></p><p>上图的图神经网络层数为2，因为计算图为2层。Layer-1、Layer-2中的神经网络层数可以为任意大小。Layer-0为节点的属性特征，是自带的，无须学习。 </p><p>图神经网络（计算图）可以任意深，但是根据“六度空间”理论，理论上已经可以得到所有信息。</p><p>图神经网络不能无限深，因为会导致所有计算图的节点都很类似，会产生过平滑（over smoothing）问题。所有节点的嵌入都会收敛到同一个值，即所有节点的embedding都相同。因此图神经网络一般两三层就足够了。</p><p>计算图的原理：</p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325154822470.png" alt="图2.2.2 两层计算图原理" style="zoom:67%;"></p><p>左图是过程，右图目标节点是最终得到的计算图</p><h3 id="2-3-K层GCC的感受野"><a href="#2-3-K层GCC的感受野" class="headerlink" title="2.3 K层GCC的感受野"></a>2.3 K层GCC的感受野</h3><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325153428719.png" alt="图2.3.1 K层GCC的感受野" style="zoom:80%;"></p><p>1-layer就是目标节点的邻居</p><p>2-layer就是目标节点邻居的邻居</p><p>3-layer就是目标节点邻居的邻居的邻居</p><p><em>层数越多，感受野越大。感受野这个词还是挺形象的</em></p><h3 id="2-4-举个例子"><a href="#2-4-举个例子" class="headerlink" title="2.4 举个例子"></a>2.4 举个例子</h3><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325155341547.png" alt="图2.4.1 无自身信息的图卷积神经网络" style="zoom:80%;"></p><p>每个节点都是128*1的向量</p><p>图卷积神经网络第一层：</p><ul><li>计算B的embedding时，A和C逐元素求平均，得到一个新的128*1维的向量。输入到黑色卷积神经网络中， 输出为512*1维度的向量。</li><li>计算C的embedding，就是计算ABEF的平均，得到128维向量，卷积后输出512维向量。</li><li>D同理。 </li></ul><p>第二层：</p><ul><li>BCD逐元素求平均，得到新的512维的向量，输入到白色卷积神经网络中，最终得到256维A的embedding，作为A节点的输出。</li></ul><h2 id="3-数学形式"><a href="#3-数学形式" class="headerlink" title="3. 数学形式"></a>3. 数学形式</h2><h3 id="3-1-数学公式表示"><a href="#3-1-数学公式表示" class="headerlink" title="3.1 数学公式表示"></a>3.1 数学公式表示</h3><script type="math/tex; mode=display">h_v^{(0)}=x_v \\h_v^{(k+1)}=\alpha(W_k\sum_{u \in N(v)}\frac{h_u^k}{|N(v)|})\\z_v=h_v^K\\</script><ul><li>第0层的属性嵌入就是每个节点的属性特征。</li><li>k+1层v节点的嵌入是由第k层邻域节点u算出来的。其实就是对k层节点求平均后输入到卷积神经网络，经过非线性激活，得到k+1层嵌入。这是一个递归的过程。</li><li>假设这个卷积神经网络只有两层，K=2，就以第二层v节点的z作为最后的输出的embedding。</li></ul><h3 id="3-2-矩阵表示"><a href="#3-2-矩阵表示" class="headerlink" title="3.2 矩阵表示"></a>3.2 矩阵表示</h3><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325164450391.png" alt="图3.2.1 矩阵表示的每一步解释" style="zoom:70%;"></p><ul><li><p>第k层所有的嵌入为$H^{(k)}=[h_1^{(k)}…h_{|V|}^{(k)}]^T$，在这个大矩阵中，每个节点的嵌入就是其中一行。</p></li><li><p>对$H$矩阵左乘一个邻接矩阵A的第v行，也就是$A_vH^{(k)}$，就可以把$v$节点的邻域节点挑出来，得到v节点的embedding。其实也就是求和的过程。</p></li><li><p>求平均的过程：找出一个$D$矩阵（一个对角矩阵）， 每个元素是对应节点的连接数。$D^{-1}AH^{(k)}$中的$D^{-1}$就是求平均的过程。</p></li><li><p>$D^{-1}A$是Row Normalized Matrix，<strong>最大特征值为1</strong>。</p></li></ul><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325170802811.png" alt="图3.2.2 计算$A_{row}$的一个例子" style="zoom:80%;"></p><p>$A_{row}=D^{-1}A$的特点：</p><ul><li>$A_{row}$是一个非对称矩阵</li><li>$A_{col}$和$A_{row}$互为转置</li><li>最大特征值为1</li><li>只按照自己的度，对所有渠道来的信息强行求平均</li><li>没有考虑对方的连接数</li></ul><p><em>因此，使用$D^{-1}A$计算权重不太科学。</em></p><p><strong>最终，我们使用$A_{sym}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$</strong></p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325184824963.png" alt="图3.2.3 $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$对应计算结果" style="zoom:80%;"></p><p><strong>$A_{sym}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$的特点：</strong></p><ul><li><strong>$A_{sym}$是一个对称矩阵</strong></li><li><strong>最大特征值为1</strong></li></ul><h3 id="3-3-tilde-A-ij-含义"><a href="#3-3-tilde-A-ij-含义" class="headerlink" title="3.3 $\tilde{A_{ij}}$含义"></a>3.3 $\tilde{A_{ij}}$含义</h3><script type="math/tex; mode=display">\tilde{A_{ij}}=\frac{1}{\sqrt{d_i}\sqrt{d_j}}</script><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325190459336.png" alt="图3.3.1 $\tilde{A_{ij}}$的两种情况" style="zoom:80%;"></p><ul><li>如果i和j直接相连，那么$\tilde{A_{ij}}$为1，此时关联程度较高</li><li>反之，关联程度较低，为$Small\tilde{A_{ij}}$</li></ul><p>$\tilde{A_{ij}}$被称为normalized diffusion matrix，有以下特征：</p><ul><li>在[-1,1]范围之间</li><li>最大值为1</li><li>$A_{sym}$其实就是$\tilde{A_{ij}}$</li><li>稀疏矩阵</li></ul><h3 id="3-4-最终版数学形式"><a href="#3-4-最终版数学形式" class="headerlink" title="3.4 最终版数学形式"></a>3.4 最终版数学形式</h3><script type="math/tex; mode=display">h_v^{(0)}=x_v \\h_v^{(k+1)}=\alpha(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}H^{(k)}W^{(k)})\\z_v=h_v^K\\</script><p>$W^{(k)}$是可学习的参数矩阵</p><p>$D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$无需学习，图一旦确定，该矩阵就已经确定</p><h2 id="4-self-edge的GCN"><a href="#4-self-edge的GCN" class="headerlink" title="4. self edge的GCN"></a>4. self edge的GCN</h2><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325192732450.png" alt="图4.1 有self edge的GCN" style="zoom:80%;"></p><p>添加了self embedding</p><h3 id="4-1-self-edge的数学形式"><a href="#4-1-self-edge的数学形式" class="headerlink" title="4.1 self edge的数学形式"></a>4.1 self edge的数学形式</h3><p>此时的数学形式为</p><script type="math/tex; mode=display">H_i^{(k)}=\alpha(\sum_{j \in \{N(i) \cup i\} }\frac{\tilde{A_{ij}}}{\sqrt{\tilde{D_{ij}}\tilde{D_{ij}}}}H_j^{(k-1)}W^{(k)})\\</script><p>或者可以写为</p><script type="math/tex; mode=display">H_i^{(k)}=\alpha(\sum_{j \in N(i)}\frac{\tilde{A_{ij}}}{\sqrt{\tilde{D_{ij}}\tilde{D_{ij}}}}H_j^{(k-1)}W^{(k)}+\frac{1}{\tilde{D_{ij}}}H_i^{(k-1)}W^{(k)})\\</script><h2 id="5-思考"><a href="#5-思考" class="headerlink" title="5. 思考"></a>5. 思考</h2><h3 id="5-1-半监督节点分类方法"><a href="#5-1-半监督节点分类方法" class="headerlink" title="5.1 半监督节点分类方法"></a>5.1 半监督节点分类方法</h3><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230325194130121.png" alt="图5.1.1 半监督节点分类方法对比" style="zoom:80%;"></p><ol><li>人工特征工程：节点重要度、集群系数、Graphlet等。</li><li>基于随机游走的方法：构造自监督表示学习任务实现图嵌入。无法泛化到新节点。例如: DeepWalk、Node2Vec、LINE、SDNE等。</li><li>标签传播：假设“物以类聚，人以群分”，利用邻域节点类别猜测当前节点类别。无法泛化到新节点。例如: Label Propagation、Iterative Classification、Belief Propagation、Correct &amp; Smooth等。</li><li>图神经网络：利用深度学习和神经网络，构造邻域节点信息聚合计算图，实现节点嵌入和类别预测。可泛化到新节点。例如: GCN、GraphSAGE、GAT、GIN等.</li></ol><h3 id="5-2-图神经网络优点"><a href="#5-2-图神经网络优点" class="headerlink" title="5.2 图神经网络优点"></a>5.2 图神经网络优点</h3><ol><li><strong>深度学习拟合学习能力强</strong>，表示学习得到的嵌入向量质量高</li><li>站在深度学习巨人肩膀上</li><li><strong>归纳式</strong>学习能力Inductive Learning：泛化到新节点、新图</li><li><strong>参数量少</strong>、所有计算图<strong>共享</strong>神经网络</li><li>利用节点<strong>属性特征</strong></li><li>利用节点<strong>标注类别</strong></li><li>可以区分节点<strong>结构功能角色</strong> (桥接、中枢、外围边缘</li><li>只需寥寥几层，就可以让任意两个节点相互影响</li></ol><h3 id="5-3-GCN是直推的还是泛化的？"><a href="#5-3-GCN是直推的还是泛化的？" class="headerlink" title="5.3 GCN是直推的还是泛化的？"></a>5.3 GCN是直推的还是泛化的？</h3><ul><li>在直推的(transductive)的场景下，GCN 的目标是对于一个给定的图（即已经有标记的节点和边），对图中的每个节点进行分类、回归等任务。在这种情况下，GCN 能够从图中已有的标记信息中学习节点的特征，然后将这些特征用于任务中。</li><li>在泛化的(inductive)的场景下，GCN 的目标是从一个训练集中学习一个模型，并将该模型泛化到不同的图上。在这种情况下，GCN 通过从训练集中学习到的节点特征和图结构，生成一个通用的模型，然后将该模型用于新的图中。</li><li>在实践中，有些 GCN 方法只能用于 transductive 的场景，而有些 GCN 方法则能够处理 inductive 的场景。例如，原始的 GCN 模型只能处理 transductive 的场景，而后来的 GraphSAGE 模型则能够处理 inductive 的场景。通常情况下，inductive GCN 模型比 transductive GCN 模型更具有实用价值，因为它们能够泛化到新的、没有见过的图中。</li></ul><h2 id="6-参考资料"><a href="#6-参考资料" class="headerlink" title="6. 参考资料"></a>6. 参考资料</h2><p><a href="https://www.bilibili.com/video/BV1Hs4y157Ls/?spm_id_from=333.788&amp;vd_source=5cf963abd0f635e31aa9385d489cf581">https://www.bilibili.com/video/BV1Hs4y157Ls/?spm_id_from=333.788&amp;vd_source=5cf963abd0f635e31aa9385d489cf581</a></p><p><a href="https://github.com/TommyZihao/zihao_course/blob/main/CS224W/6-3-GCN.md">https://github.com/TommyZihao/zihao_course/blob/main/CS224W/6-3-GCN.md</a></p>]]></content>
      
      
      <categories>
          
          <category> Graph Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GCN </tag>
            
            <tag> Graph Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GRU</title>
      <link href="/2023/03/26/gru/"/>
      <url>/2023/03/26/gru/</url>
      
        <content type="html"><![CDATA[<h1 id="GRU-门控循环单元"><a href="#GRU-门控循环单元" class="headerlink" title="GRU 门控循环单元"></a>GRU 门控循环单元</h1><p><em>RNN无法处理较长数据，所有信息都在隐藏状态中，时间较长时累计状态过多，之前的信息不好做抽取。</em></p><p><strong>GRU思想：不是每个观察值都同等重要</strong></p><p><strong>通过门控单元，选择重要的内容 忘记不重要的内容</strong></p><h2 id="1-门控隐状态"><a href="#1-门控隐状态" class="headerlink" title="1.门控隐状态"></a>1.门控隐状态</h2><p>门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。</p><p>门控循环单元模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 </p><p><strong>隐状态三种情况：</strong></p><ol><li>如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 </li><li>同样，模型也可以学会跳过不相关的临时观测。</li><li>最后，模型还将学会在需要的时候重置隐状态。</li></ol><h3 id="1-1重置门和更新门"><a href="#1-1重置门和更新门" class="headerlink" title="1.1重置门和更新门"></a>1.1重置门和更新门</h3><p>我们把更新门和重置门设计成(0,1)区间中的向量，进行凸组合</p><ul><li><strong>更新门</strong> 能关注的机制 控制“可能还想记住”的过去状态的数量</li><li><strong>重置门（遗忘门）</strong> 能遗忘的机制 控制新状态中有多少个是旧状态的副本</li></ul><p>下图描述门控循环单元中的重置门和更新门的输入， 输入是由当前时间步的输入和前一时间步的隐状态给出。 两个门的输出是由使用sigmoid激活函数的两个全连接层给出。</p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230321171826823.png" alt="" style="zoom: 80%;"></p><p>对于时间步$t$，假设输入是一个小批量$X_t \in \mathbb {R} ^{n*d}$ (样本个数$n$，输入个数$d$)，</p><p>上一步时间步的隐状态是 $H_{t-1}\in\mathbb{R}^{n*h}$ （隐藏单元个数$h$）。</p><p>重置门 $R_t\in\mathbb{R}^{n*h}$</p><p>更新门  $Z_t\in\mathbb{R}^{n*h}$ </p><p>计算如下所示：</p><script type="math/tex; mode=display">R_t=\alpha(X_tW_{xr}+H_{t-1}W_{hr}+b_r) \tag{1.1.1} \\ Z_t=\alpha(X_tW_{xz}+H_{t-1}W_{hz}+b_z)</script><p>$W_{xr},W_{xz} \in \mathbb{R}^{d*h}$ </p><p>$W_{hr},W_{hz} \in \mathbb{R}^{h*h}$是权重参数</p><p>$b_r，b_z \in \mathbb{R}^{1*h}$是偏置参数</p><p><strong>在求和过程中会触发广播机制。</strong>同时使用sigmoid函数将输入值转换到区间$(0,1)$</p><h3 id="1-2候选隐状态"><a href="#1-2候选隐状态" class="headerlink" title="1.2候选隐状态"></a>1.2候选隐状态</h3><p>我们将重置门$R_t$与RNN中的常规隐状态更新机制集成，得到在时间步$t$的候选隐状态$\tilde{H}_t \in \mathbb {R}^{n*h}$。</p><script type="math/tex; mode=display">\tilde{H}_t = tanh(X_tW_{xh}+(R_t \odot H_{t-1})W_{hh}+b_h) \tag{1.2.1}</script><p>$W_{xh}\in\mathbb{R}^{d*h}$ </p><p>$W_{hh} \in \mathbb{R}^{h*h}$ 是权重参数</p><p>$b_h \in \mathbb {R}^{1*h}$是偏置项</p><p>符号 $\odot$ 是Hadamard积（按照元素乘积）运算符</p><p>在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1，1)$中。</p><p>与RNN的常规隐状态相比，$(1.2.1)$中的$R_t$和$H_{t-1}$的元素相乘可以减少以往状态的影响。</p><p><strong>每当重置门$R_t$中的项接近1时，我们恢复一个例如RNN的常规隐状态的普通循环神经网络。</strong></p><p><strong>对于重置门$R_t$中所有接近0的项，候选隐状态是以$X_t$作为输入的多层感知机的结果。</strong></p><p>因此，任何预先存在的隐状态都会被重置为默认值。</p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230322104641189.png" alt="" style="zoom:67%;"></p><p>上图说明了应用重置门之后的计算流程。</p><h3 id="1-3隐状态"><a href="#1-3隐状态" class="headerlink" title="1.3隐状态"></a>1.3隐状态</h3><p>上述的计算结果只是候选隐状态，我们仍然需要结合更新门$Z_t$的效果。 </p><p><strong>这一步确定新的隐状态$H_t \in \mathbb{R}^{n*h}$在多大程度上来自旧的状态$H_{t-1}$和新的候选状态$\tilde{H}_t$。</strong></p><p><strong>更新门$Z_t$仅需要在$H_{t-1}$和$H_{t}$之间进行按元素的凸组合就可以实现这个目标。</strong> </p><p>这就得出了门控循环单元的最终更新公式：</p><script type="math/tex; mode=display">H_t = Z_t \odot H_{t-1}+(1-Z_t) \odot \tilde{H}_t \tag{1.3.1}</script><p><strong>每当更新门$Z_t$接近1时，模型就倾向只保留旧状态。</strong> </p><p><strong>此时，来自$X_t$的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步$t$。</strong></p><p><strong>相反，当$Z_t$接近0时， 新的隐状态$H_t$就会接近候选隐状态$\tilde{H}_t$。</strong></p><p><strong>这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。</strong> </p><p>例如，如果整个子序列的所有时间步的更新门都接近于1，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230322110327072.png" alt="" style="zoom:80%;"></p><p>上图说明了更新门起作用后的计算流。</p><h2 id="2-特征"><a href="#2-特征" class="headerlink" title="2.特征"></a>2.特征</h2><p>GRU有以下俩个显著特征：</p><ul><li><strong>重置门有助于捕获序列中的短期依赖关系</strong></li><li><strong>更新门有助于捕获序列中的长期依赖关系</strong></li></ul><h2 id="3-参考网站"><a href="#3-参考网站" class="headerlink" title="3.参考网站"></a>3.参考网站</h2><p><a href="https://www.bilibili.com/video/BV1mf4y157N2/?spm_id_from=333.999.0.0&amp;vd_source=5cf963abd0f635e31aa9385d489cf581">https://www.bilibili.com/video/BV1mf4y157N2/?spm_id_from=333.999.0.0&amp;vd_source=5cf963abd0f635e31aa9385d489cf581</a></p><p><a href="https://zh-v2.d2l.ai/chapter_recurrent-modern/gru.html#id4">https://zh-v2.d2l.ai/chapter_recurrent-modern/gru.html#id4</a></p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GRU </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM</title>
      <link href="/2023/03/26/lstm/"/>
      <url>/2023/03/26/lstm/</url>
      
        <content type="html"><![CDATA[<h1 id="LSTM-长短期记忆网络"><a href="#LSTM-长短期记忆网络" class="headerlink" title="LSTM 长短期记忆网络"></a>LSTM 长短期记忆网络</h1><p><strong>长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。</strong> </p><p>解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）。</p><h2 id="1-实现效果"><a href="#1-实现效果" class="headerlink" title="1. 实现效果"></a>1. 实现效果</h2><ul><li><p>忘掉过去状态，尽量去看现在的输入数据</p></li><li><p>不看现在的输入数据，尽量去看前一个的状态</p></li></ul><h2 id="2-模型组成"><a href="#2-模型组成" class="headerlink" title="2. 模型组成"></a>2. 模型组成</h2><h3 id="2-1门控记忆单元"><a href="#2-1门控记忆单元" class="headerlink" title="2.1门控记忆单元"></a>2.1门控记忆单元</h3><p>可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。</p><p>长短期记忆网络引入了<strong>记忆元</strong>，或简称为单元（cell）。有些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态具有相同的形状，其<strong>设计目的是用于记录附加的信息</strong>。</p><p>为了控制记忆元，我们需要许多门。<strong>其中一个门用来从单元中输出条目，我们将其称为输出门（output gate）。另外一个门用来决定何时将数据读入单元，我们将其称为输入门（input gate）。</strong></p><p>我们还需要一种机制来<strong>重置单元的内容，由遗忘门（forget gate）来管理</strong>， 这种设计的动机与门控循环单元相同，能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。</p><ul><li><p>遗忘门：将值朝0减少</p></li><li><p>输入门：决定不是忽略掉输入数据</p></li><li><p>输出门：决定是不是使用隐状态</p></li></ul><h3 id="2-2输入门、忘记门和输出门"><a href="#2-2输入门、忘记门和输出门" class="headerlink" title="2.2输入门、忘记门和输出门"></a>2.2输入门、忘记门和输出门</h3><p>就如在门控循环单元中一样， 当前时间步的输入和前一个时间步的隐状态作为数据送入长短期记忆网络的门中， 如图所示。它们由三个具有sigmoid激活函数的全连接层处理，以计算输入门、遗忘门和输出门的值。因此，这三个门的值都在(0,1)的范围内。</p><p><img src="http://yanhaoli-images.oss-cn-beijing.aliyuncs.com/img/image-20230322113226798.png" alt="" style="zoom:80%;"></p><p><strong>长短期记忆网络的数学表达。</strong></p><p>假设有$ℎ$个隐藏单元，批量大小为$n$，输入数为$d$。 </p><p>输入为$X_t \in \mathbb {R}^{n*d}$</p><p>前一时间步的隐状态为$H_{t-1} \in \mathbb {R}^{n*h}$。 </p><p>相应地，时间步$t$的门被定义如下：</p><p>输入门是$I_t \in \mathbb {R}^{n*h}$ </p><p>遗忘门是$F_t \in \mathbb {R}^{n*h}$</p><p> 输出门是$O_t \in \mathbb {R}^{n*h}$。</p><p>它们的计算方法如下：</p><script type="math/tex; mode=display">I_t=\alpha(X_tW_{xi}+H_{t-1}W_{hi}+b_i)\\F_t=\alpha(X_tW_{xf}+H_{t-1}W_{hf}+b_f)\\O_t=\alpha(X_tW_{xo}+H_{t-1}W_{ho}+b_o)\\ \tag{2.2.1}</script>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
